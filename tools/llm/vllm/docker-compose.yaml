---

# Reasoning:
#   - Qwen/Qwen3-14B-AWQ
# Instruct:
#   - Qwen/Qwen2.5-14B-Instruct-AWQ
#   - Qwen/Qwen2.5-Coder-14B-Instruct-AWQ
#   - TheBloke/CodeLlama-13B-Instruct-AWQ
#   - deepseek-ai/deepseek-coder-6.7b-instruct
# Coding:
#   - Qwen/Qwen2.5-Coder-7B
#   - TheBloke/CodeLlama-13B-AWQ
#   - TheBloke/CodeLlama-13B-Python-AWQ

services:

  vllm-qwen-coder:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen-coder
    restart: unless-stopped
 
    ports:
      - "8000:8000"
 
    environment:
      # Hugging Face token (optional, only if you need private models)
      # - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - NCCL_P2P_DISABLE=1  # Disable P2P for single GPU stability
 
    volumes:
      # Cache model downloads
      - $HOME/.docker/volumes/vllm-cache:/root/.cache/huggingface
      # Optional: Mount local model directory if you pre-downloaded
      # - ./local_models:/models
 
    command:
      - --model=Qwen/Qwen2.5-Coder-7B-Instruct
      - --host=0.0.0.0
      - --port=8000
      # GPU and memory settings
      - --gpu-memory-utilization=0.9
      - --max-model-len=8192
      # Performance optimizations for autocompletion
      - --enable-prefix-caching
      - --enable-chunked-prefill
      - --max-num-seqs=32
      - --max-num-batched-tokens=8192
      # FIM (Fill-in-Middle) support for code completion
      - --enable-auto-tool-choice
      - --tool-call-parser=hermes
      # API settings
      - --served-model-name=qwen-coder-7b
      - --trust-remote-code
 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]
 
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  models:
    driver: local

