---

# Reasoning:
#   - Qwen/Qwen3-14B-AWQ
# Instruct:
#   - Qwen/Qwen2.5-14B-Instruct-AWQ
#   - Qwen/Qwen2.5-Coder-14B-Instruct-AWQ
#   - TheBloke/CodeLlama-13B-Instruct-AWQ
#   - deepseek-ai/deepseek-coder-6.7b-instruct
# Coding:
#   - Qwen/Qwen2.5-Coder-7B
#   - TheBloke/CodeLlama-13B-AWQ
#   - TheBloke/CodeLlama-13B-Python-AWQ

services:
  vllm-qwen-coder-chat:
    image: vllm/vllm-openai:v0.12.0
    container_name: vllm-qwen-coder-chat
    restart: unless-stopped
 
    ports:
      - "8001:8000"
 
    environment:
      # CRITICAL: Don't set CUDA_VISIBLE_DEVICES - let vLLM see all GPUs
      # NCCL settings for multi-GPU communication
      - NCCL_DEBUG=INFO
      - NCCL_IB_DISABLE=1
      - NCCL_P2P_LEVEL=NVL
      - NCCL_SOCKET_IFNAME=lo
      - NCCL_TIMEOUT=1800
      # Ray configuration for multi-GPU
      - RAY_DEDUP_LOGS=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn

    volumes:
      - $HOME/.docker/volumes/vllm-cache:/root/.cache/huggingface

    shm_size: 8gb

    command:
      - --model=Qwen/Qwen2.5-Coder-32B-Instruct-AWQ
      - --host=0.0.0.0
      - --port=8000
      - --quantization=awq
      # Tensor parallelism across 2 GPUs
      #- --tensor-parallel-size=2
      # GPU and memory settings
      - --gpu-memory-utilization=0.90
      - --max-model-len=16384
      # Performance optimizations
      - --disable-log-requests
      # API settings
      - --served-model-name=qwen-coder-32b
      - --trust-remote-code
  
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
 
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Longer startup for larger model

  vllm-qwen-coder:
    image: vllm/vllm-openai:v0.12.0
    container_name: vllm-qwen-coder
    restart: unless-stopped
 
    ports:
      - "8000:8000"
 
    environment:
      # Hugging Face token (optional, only if you need private models)
      # - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - NCCL_P2P_DISABLE=1  # Disable P2P for single GPU stability
 
    volumes:
      # Cache model downloads
      - $HOME/.docker/volumes/vllm-cache:/root/.cache/huggingface
      # Optional: Mount local model directory if you pre-downloaded
      # - ./local_models:/models
 
    command:
      - --model=Qwen/Qwen2.5-Coder-7B-Instruct
      - --host=0.0.0.0
      - --port=8000
      # GPU and memory settings
      - --gpu-memory-utilization=0.9
      - --max-model-len=8192
      # Performance optimizations for autocompletion
      - --enable-prefix-caching
      - --enable-chunked-prefill
      - --max-num-seqs=32
      - --max-num-batched-tokens=8192
      # FIM (Fill-in-Middle) support for code completion
      - --enable-auto-tool-choice
      - --tool-call-parser=hermes
      # API settings
      - --served-model-name=qwen-coder-7b
      - --trust-remote-code
 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
 
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  models:
    driver: local

