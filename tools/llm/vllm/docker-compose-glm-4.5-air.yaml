---

services:

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    ports:
      - 3000:8080
    environment:
      - OPENAI_API_BASE_URL=http://vllm-glm-4.5-air:8000/v1
    restart: unless-stopped

  vllm-glm-4.5-air:
    image: vllm/vllm-openai:nightly
    restart: unless-stopped
 
    ports:
      - "8003:8000"
 
    environment:
      # CRITICAL: Don't set CUDA_VISIBLE_DEVICES - let vLLM see all GPUs
      # NCCL settings for multi-GPU communication
      - NCCL_DEBUG=INFO
      - NCCL_IB_DISABLE=1
      - NCCL_P2P_LEVEL=NVL
      - NCCL_SOCKET_IFNAME=lo
      - NCCL_TIMEOUT=1800
      # Ray configuration for multi-GPU
      - RAY_DEDUP_LOGS=0
      - VLLM_WORKER_MULTIPROC_METHOD=spawn

    volumes:
      - $HOME/.cache/huggingface:/root/.cache/huggingface

    shm_size: 8gb

    command:
      - cyankiwi/GLM-4.5-Air-AWQ-4bit
      - --host=0.0.0.0
      - --port=8000
      - --served-model-name=glm-4.5-air
      - --tensor-parallel-size=4
      - --speculative-config.method=mtp
      - --speculative-config.num_speculative_tokens=1
      - --tool-call-parser=glm45
      - --reasoning-parser=glm45
      - --enable-auto-tool-choice
      - --max-model-len=65536
      - --max-num-batched-tokens=32768
      - --gpu-memory-utilization=0.95
 
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0','1','2','3']
              capabilities: [gpu]
 
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Longer startup for larger model

volumes:
  models:
    driver: local

