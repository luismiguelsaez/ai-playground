---

services:

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    ports:
      - 3001:8080
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
    restart: unless-stopped

  vllm:
    image: vllm/vllm-openai:nightly
    restart: unless-stopped
 
    ports:
      - "8004:8000"
 
    environment:
      - CUDA_VISIBLE_DEVICES=0,1,2,3

    volumes:
      - $HOME/.cache/huggingface:/root/.cache/huggingface
      - $PWD/config/models:/configs

    shm_size: 8gb

    command:
      - --config
      - /configs/glm4.7-flash.yaml

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0','1','2','3']
              capabilities: [gpu]
 
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Longer startup for larger model

volumes:
  models:
    driver: local

